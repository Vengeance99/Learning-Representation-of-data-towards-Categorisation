{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDEC_MNIST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vengeance99/Learning-Representation-of-data-towards-Categorisation/blob/main/IDEC_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKA8-yKK466I"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r7sJPuVRshY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.empty((512,512))"
      ],
      "metadata": {
        "id": "bIoUMUin-Bq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x)):\n",
        "  for j in range(len(x)):\n",
        "    if i==j:\n",
        "       x[i][j]=1"
      ],
      "metadata": {
        "id": "8Rmnh3UC-G1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "# from barbar import Bar\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import recall_score\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline\n",
        "# %matplotlib inline\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import gc\n",
        "RANDOMSTATE = 0\n",
        "import os"
      ],
      "metadata": {
        "id": "juwRgrA2sZfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "CaCjNS-W-jsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "T9WewzGPelmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "\n",
        "\n",
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "\n",
        "train_set = dset.FashionMNIST(root='/content/drive/MyDrive', train=True, transform=trans, download=True)\n",
        "test_set = dset.FashionMNIST(root='/content/drive/MyDrive', train=False, transform=trans, download=True)\n",
        "batch_size = 100\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)"
      ],
      "metadata": {
        "id": "Td1wL7XS-8X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader))"
      ],
      "metadata": {
        "id": "89Nooht2gIBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "timingResult = {}\n",
        "def logTime(theName, currentTime):\n",
        "    if theName not in timingResult:\n",
        "        timingResult[theName] = time.time() - currentTime\n",
        "    else:\n",
        "        timingResult[theName] = timingResult[theName] + (time.time() - currentTime)\n",
        "    currentTime = time.time()\n",
        "    return currentTime\n",
        "\n",
        "def printTiming(name):\n",
        "    print('======== timing for {}: {} ======='.format(name,timingResult[name]))"
      ],
      "metadata": {
        "id": "m-n1heTBiHt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DEC_AE(nn.Module):\n",
        "    def __init__(self, num_classes, num_features):\n",
        "        super(DEC_AE,self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.fc1 = nn.Linear(28*28,500)\n",
        "        self.fc2 = nn.Linear(500,500)\n",
        "        self.fc3 = nn.Linear(500,2000)\n",
        "        self.fc4 = nn.Linear(2000,num_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_d1 = nn.Linear(500,28*28)\n",
        "        self.fc_d2 = nn.Linear(500,500)\n",
        "        self.fc_d3 = nn.Linear(2000,500)\n",
        "        self.fc_d4 = nn.Linear(num_features,2000)\n",
        "        self.alpha = 1.0\n",
        "        # self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features))\n",
        "        # self.pretrainMode = True\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform(m.weight)\n",
        "\n",
        "    # def setPretrain(self,mode):\n",
        "    #     \"\"\"To set training mode to pretrain or not, \n",
        "    #     so that it can control to run only the Encoder or Encoder+Decoder\"\"\"\n",
        "    #     self.pretrainMode = mode\n",
        "    # def updateClusterCenter(self, cc):\n",
        "    #     \"\"\"\n",
        "    #     To update the cluster center. This is a method for pre-train phase.\n",
        "    #     When a center is being provided by kmeans, we need to update it so\n",
        "    #     that it is available for further training\n",
        "    #     :param cc: the cluster centers to update, size of num_classes x num_features\n",
        "    #     \"\"\"\n",
        "    #     self.clusterCenter.data = torch.from_numpy(cc)\n",
        "    # def getTDistribution(self,x, clusterCenter):\n",
        "    #     \"\"\"\n",
        "    #     student t-distribution, as same as used in t-SNE algorithm.\n",
        "    #      q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "         \n",
        "    #      :param x: input data, in this context it is encoder output\n",
        "    #      :param clusterCenter: the cluster center from kmeans\n",
        "    #      \"\"\"\n",
        "    #     xe = torch.unsqueeze(x,1).cuda() - clusterCenter.cuda()\n",
        "    #     q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
        "    #     q = q ** (self.alpha + 1.0) / 2.0\n",
        "    #     q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
        "    #     return q\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = x.view(-1, 1*28*28)\n",
        "        # 32x32x1\n",
        "        x = self.dropout(x)\n",
        "        # 32x32x1\n",
        "        x = self.fc1(x)\n",
        "        # 17x17x50\n",
        "        x = self.relu(x)\n",
        "        # 17x17x50\n",
        "        x = self.fc2(x)\n",
        "        # 17x17x50\n",
        "        x = self.relu(x)\n",
        "        # 9x9x50\n",
        "        x = self.fc3(x)\n",
        "        # 17x17x50\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        # 9x9x50\n",
        "        x_ae = x\n",
        "        #if not in pretrain mode, we only need encoder\n",
        "        # if self.pretrainMode == False:\n",
        "        #     return x, self.getTDistribution(x,self.clusterCenter)\n",
        "        # 1x68\n",
        "        ##### encoder is done, followed by decoder #####\n",
        "        # 1x68\n",
        "        x = self.fc_d4(x)\n",
        "        # 1x4050\n",
        "        x = self.relu(x)\n",
        "        # 1x4050\n",
        "        x = self.fc_d3(x)\n",
        "        # 1x4050\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_d2(x)\n",
        "        # 1x4050\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_d1(x)\n",
        "        x_de = x.view(-1,1,28,28)\n",
        "        # 1x4050\n",
        "        return x_ae, x_de"
      ],
      "metadata": {
        "id": "xXQZoIaggMq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Require scikit-learn installed\n",
        "    # Arguments\n",
        "        y: true labels, numpy.array with shape `(n_samples,)`\n",
        "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "    # Return\n",
        "        accuracy, in [0,1]\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    # from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "    # for i,j in ind:\n",
        "    ind=list(ind)\n",
        "    print(ind[0])\n",
        "    # print(i for )\n",
        "    return (sum([w[i, j] for i, j in zip(ind[0],ind[1])]) * 1.0 / y_pred.size)"
      ],
      "metadata": {
        "id": "9TcQ48VKhJBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class DEC:\n",
        "#     \"\"\"The class for controlling the training process of DEC\"\"\"\n",
        "#     def __init__(self,n_clusters,alpha=1.0):\n",
        "#         self.n_clusters=n_clusters\n",
        "#         self.alpha = alpha\n",
        "        \n",
        "\n",
        "#     def target_distribution(q):\n",
        "#         weight = q ** 2 / q.sum(0)\n",
        "#         #print('q',q)\n",
        "#         return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)\n",
        "#     def logAccuracy(self,pred,label):\n",
        "#         print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
        "#           % (acc(label, pred), nmi(label, pred)))\n",
        "\n",
        "#     def kld(q,p):\n",
        "#         res = torch.sum(p*torch.log(p/q),dim=-1)\n",
        "#         return res\n",
        "    \n",
        "def validateOnCompleteTestData(test_loader,model):\n",
        "    model.eval()\n",
        "    lat=[]\n",
        "    lab=[]\n",
        "    for i,(data,label )in enumerate(test_loader):\n",
        "            data=data.to(device)\n",
        "            label=label.to(device)\n",
        "            x_ae,_ = model(data)\n",
        "            lat.extend(x_ae.data.cpu().numpy())\n",
        "            lab.extend(label.cpu().numpy())\n",
        "    to_eval=np.array(lat)\n",
        "    true_labels=np.array(lab)\n",
        "\n",
        "            # to_eval=np.array()\n",
        "    # to_eval = np.array([model(d[0].cuda())[0].data.cpu().numpy() for i,d in enumerate(test_loader)])\n",
        "    # true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])\n",
        "    print(to_eval.shape)\n",
        "    print(true_labels.shape)\n",
        "    # to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))\n",
        "    # true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])\n",
        "    km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20)\n",
        "    y_pred = km.fit_predict(to_eval)\n",
        "    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
        "                  % (acc(true_labels, y_pred), nmi(true_labels, y_pred)))\n",
        "    currentAcc = acc(true_labels, y_pred)\n",
        "    return currentAcc\n",
        "    \n",
        "def pretrain(train_loader, test_loader, epochs):\n",
        "    dec_ae = DEC_AE(10,10).cuda() #auto encoder\n",
        "    mseloss = nn.MSELoss()\n",
        "    optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        dec_ae.train()\n",
        "        running_loss=0.0\n",
        "        for i,(data,label )in enumerate(train_loader):\n",
        "            # print(type(data))\n",
        "            data=data.to(device)\n",
        "            label=label.to(device)\n",
        "            # x, label = data\n",
        "            # x, label = Variable(x).cuda(),Variable(label).cuda()\n",
        "            optimizer.zero_grad()\n",
        "            x_ae,x_de = dec_ae(data)\n",
        "            loss = F.mse_loss(x_de,data,reduce=True) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # x_eval = x.data.cpu().numpy()\n",
        "            # label_eval = label.data.cpu().numpy()\n",
        "            # running_loss += loss.data.cpu().numpy()\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "            if i % 100 == 99:    # print every 100 mini-batches\n",
        "                print('[%d, %5d] loss: %.7f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "        #now we evaluate the accuracy with AE\n",
        "        dec_ae.eval()\n",
        "        currentAcc =validateOnCompleteTestData(test_loader,dec_ae)\n",
        "        if currentAcc > best_acc:                \n",
        "            torch.save(dec_ae,'/content/drive/MyDrive/my_models/bestModel'.format(best_acc))\n",
        "            best_acc = currentAcc\n",
        "    #     # print(f\"loss for epoch{i}:\")        \n",
        "    # def clustering(self,mbk,x,model):\n",
        "    #     model.eval()\n",
        "    #     y_pred_ae,_ = model(x)\n",
        "    #     y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
        "    #     y_pred = mbk.partial_fit(y_pred_ae) #seems we can only get a centre from batch\n",
        "    #     self.cluster_centers = mbk.cluster_centers_ #keep the cluster centers\n",
        "    #     model.updateClusterCenter(self.cluster_centers)\n",
        "    # def train(self,train_loader, test_loader, epochs):\n",
        "    #     \"\"\"This method will start training for DEC cluster\"\"\"\n",
        "    #     ct = time.time()\n",
        "    #     dec_ae = DEC_AE(10,10).cuda() \n",
        "    #     best_acc = 0.0\n",
        "    #     model = torch.load(\"/content/drive/MyDrive/my_models/bestModel\").cuda()\n",
        "    #     model.setPretrain(False)\n",
        "    #     optimizer = optim.SGD([\\\n",
        "    #          {'params': model.parameters()}, \\\n",
        "    #         ],lr = 0.01, momentum=0.9)\n",
        "    #     print('Initializing cluster center with pre-trained weights')\n",
        "    #     mbk = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=batch_size)\n",
        "    #     got_cluster_center = False\n",
        "    #     for epoch in range(epochs):\n",
        "    #         for i,(data,label) in enumerate(train_loader):\n",
        "    #             # x, label = data\n",
        "    #             # x = Variable(x).cuda()\n",
        "    #             data=data.to(device)\n",
        "    #             label=label.to(device)\n",
        "    #             optimizer.zero_grad()\n",
        "    #             # data=np.array(data)\n",
        "    #             # data=torch.from_numpy(data)\n",
        "    #             # data=data.to(device)\n",
        "\n",
        "    #             #step 1 - get cluster center from batch\n",
        "    #             #here we are using minibatch kmeans to be able to cope with larger dataset.\n",
        "    #             if not got_cluster_center:\n",
        "    #                 self.clustering(mbk,data,model)\n",
        "    #                 if epoch > 1:\n",
        "    #                     got_cluster_center = True\n",
        "    #             else:\n",
        "    #                 model.train()\n",
        "    #                 #now we start training with acquired cluster center\n",
        "    #                 feature_pred,q = model(data)\n",
        "\n",
        "    #                 #get target distribution\n",
        "    #                 p = self.target_distribution(q)\n",
        "    #                 #print('q',q,'p',p)\n",
        "    #                 loss1 = self.kld(q,p).mean()\n",
        "    #                 x_ae,x_de = dec_ae(data)\n",
        "    #                 loss2 = F.mse_loss(x_de,data,reduce=True) \n",
        "    #                 loss=loss1+loss2\n",
        "    #                 loss.backward()\n",
        "    #                 optimizer.step()\n",
        "    #         currentAcc = self.validateOnCompleteTestData(test_loader,model)\n",
        "    #         if currentAcc > best_acc:                \n",
        "    #             torch.save(model,'/content/drive/MyDrive/my_models/DECbestModel'.format(best_acc))\n",
        "    #             best_acc = currentAcc"
      ],
      "metadata": {
        "id": "WSBGI9gWhWxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now start training\n",
        "# import random\n",
        "# random.seed(7)\n",
        "# dec = DEC(10)\n",
        "pretrain(train_loader, test_loader, 200)\n",
        "# dec.train(train_loader, test_loader, 100)"
      ],
      "metadata": {
        "id": "pApanKwRh9MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =DEC_AE(10,10).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/MINI_MODELS/Final_model_conv.pt', map_location=device)['model_state_dict'], strict=False)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "FLjgwKDliAtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SW3_mUY0vQZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}